{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "662da44a",
      "metadata": {},
      "source": [
        "# Exercise 3\n",
        "### Computer Vision, Fall 2025\n",
        "#### Name: <font color='blue'>Matias Paloranta</font>\n",
        "---\n",
        "**Instructions:**\n",
        "- Return the answer in PDF and Jupyter Notebook formats.\n",
        "\n",
        "- Return latest on<font color='red'> Sunday Sep. 28 at 23.50</font> via Moodle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04ddc4bc-9959-4401-bcdf-67805939d106",
      "metadata": {},
      "source": [
        "## Ex 3.1. CAMERA MATRIX (4 p)\n",
        "Please find the 3 sets of images provided in the Moodle page of this week's exercise. The three image sets (A,B and C) contain pictures taken of a camera calibration checkerboard pattern with a square size of 3cm.\n",
        "\n",
        "1. Write a function using built-in OpenCV methods that computes the intrinsic camera calibration matrix and distortion matrix from a given set of calibration images.\n",
        "\n",
        "2. Apply that function to all three sets of images and observe the results. Based on the results for the intrinsic and distortion matrix, discuss what type of camera or lens was used to capture the different image sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "cc269a8a-57a3-459f-a302-bb36dc894fc0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images A:\n",
            "cx,cy: 1623.5545460691144 1860.4396769537173\n",
            "center: 1512.0 2016.0\n",
            "total error: 0.15474618043978428\n",
            "[[3553.30638915    0.         1623.55454607]\n",
            " [   0.         3552.90679709 1860.43967695]\n",
            " [   0.            0.            1.        ]]\n",
            "[[ 0.12107222 -0.08137645 -0.01189253  0.01028686  0.        ]]\n",
            "\n",
            "Images B:\n",
            "cx,cy: 1477.3725415746737 1989.9342891072497\n",
            "center: 1512.0 2016.0\n",
            "total error: 0.21089247090030022\n",
            "[[1706.66147505    0.         1477.37254157]\n",
            " [   0.         1703.3379684  1989.93428911]\n",
            " [   0.            0.            1.        ]]\n",
            "[[-0.00245321  0.01280024 -0.00171085 -0.00364829  0.        ]]\n",
            "\n",
            "Images C:\n",
            "cx,cy: 889.0180734800169 1573.2251133885334\n",
            "center: 1512.0 2016.0\n",
            "total error: 0.24184582728248252\n",
            "[[7802.99246987    0.          889.01807348]\n",
            " [   0.         7751.04024232 1573.22511339]\n",
            " [   0.            0.            1.        ]]\n",
            "[[ 0.20523726  0.1403325  -0.03772315 -0.03353585  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "\n",
        "def calibrate(img_path):\n",
        "    matrix = None\n",
        "    distortion = None\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
        "    points_row, points_col = (6, 8) # number of inner corners is number of squares - 1 in any given dimension\n",
        "    square_size = 0.03 # in meters\n",
        "    objp = np.zeros((points_col*points_row, 3), np.float32)\n",
        "    objp[:,:2] = np.mgrid[0:points_row, 0:points_col].T.reshape(-1,2) * square_size\n",
        " \n",
        "    # Arrays to store object points and image points from all the images.\n",
        "    objpoints = [] # 3d point in real world space\n",
        "    imgpoints = [] # 2d points in image plane.\n",
        "\n",
        "    images = glob.glob(\"{}/*.jpeg\".format(img_path))\n",
        "\n",
        "    for filename in images:\n",
        "        image = cv2.imread(filename)\n",
        "        grayColor = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "         \n",
        "        # Find the chess board corners\n",
        "        ret, corners = cv2.findChessboardCornersSB(grayColor, (points_row, points_col), None)\n",
        "    \n",
        "        # If found, add object points, image points (after refining them)\n",
        "        if ret == True:\n",
        "            objpoints.append(objp)\n",
        "    \n",
        "            corners2 = cv2.cornerSubPix(grayColor, corners, (11,11), (-1,-1), criteria)\n",
        "            imgpoints.append(corners2)\n",
        "            \n",
        "    #         cv2.drawChessboardCorners(image, (points_row, points_col), corners2, ret)\n",
        "    #         cv2.namedWindow(\"img\", cv2.WINDOW_NORMAL)\n",
        "    #         cv2.imshow(\"img\", image)\n",
        "    #         cv2.waitKey(0)\n",
        "    # cv2.destroyAllWindows()\n",
        "        \n",
        "    flags = cv2.CALIB_FIX_K3 | cv2.CALIB_FIX_K4 | cv2.CALIB_FIX_K5\n",
        "    ret, matrix, distortion, rvecs, tvecs = cv2.calibrateCamera(\n",
        "        objpoints, imgpoints, grayColor.shape[::-1], None, None, flags=flags\n",
        "    )\n",
        "    \n",
        "    W, H = 3024, 4032\n",
        "    print(\"cx,cy:\", matrix[0,2], matrix[1,2])\n",
        "    print(\"center:\", W/2, H/2)\n",
        "    \n",
        "    mean_error = 0\n",
        "    for i in range(len(objpoints)):\n",
        "        imgpoints2, _ = cv2.projectPoints(objpoints[i], rvecs[i], tvecs[i], matrix, distortion)\n",
        "        error = cv2.norm(imgpoints[i], imgpoints2, cv2.NORM_L2)/len(imgpoints2)\n",
        "        mean_error += error\n",
        "    \n",
        "    print( \"total error: {}\".format(mean_error/len(objpoints)) )\n",
        "    return matrix, distortion\n",
        "\n",
        "\n",
        "print(\"Images A:\")\n",
        "matrix, distortion = calibrate(\"images_A\")\n",
        "print(matrix)\n",
        "print(distortion)\n",
        "print()\n",
        "\n",
        "print(\"Images B:\")\n",
        "matrix, distortion = calibrate(\"images_B\")\n",
        "print(matrix)\n",
        "print(distortion)\n",
        "print()\n",
        "\n",
        "print(\"Images C:\")\n",
        "matrix, distortion = calibrate(\"images_C\")\n",
        "print(matrix)\n",
        "print(distortion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a2c660e-8752-4c6a-afe2-077b01186e71",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5dfdf103-ef14-4321-90dd-ee49c3ed3073",
      "metadata": {},
      "source": [
        "## Ex 3.2. HOUGH TRANSFORM (4 p)\n",
        "1. Ask ChatGPT to explain the main points about Hough Transform using the key word \"Hough Transform\". Analyse the answer by reflecting it to the course material related to Hough Transform: Were all main points in the course material addressed? if no, please shortly conclude the answers of ChatGPT and our course materials regarding the Hough Transform, and list the differences of answers between our course material and ChatGPT.\n",
        "2. We provided you example images from the Berkeley Segmentation Dataset. Implement the Hough Transform (the most basic version) for line detection by yourself (You may only use the openCV function to check if your solution is correct). Instead of using the default parameters, customize the parameters (e.g., resolution of the parameter space, threshold values for line detection, minimum line length, maximum line gap) to optimize edge detection for the given set of images. Report your observations: which parameters influenced which behaviour in the output? Also report which parameter configuration resembles closest an object segmentation in the test images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7107da6-a27d-4117-a0f1-20aa5fd364c8",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#2)\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "def apply_hough_transform(image):\n",
        "    ...\n",
        "\n",
        "\n",
        "images = glob.glob(\"segmentation_images/*.jpg\")\n",
        "\n",
        "for filename in images:\n",
        "    image = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61aab545-be21-4abf-81a2-559d0fccb300",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
