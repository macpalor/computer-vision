{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "662da44a",
      "metadata": {},
      "source": [
        "# Exercise 3\n",
        "### Computer Vision, Fall 2025\n",
        "#### Name: <font color='blue'>Matias Paloranta</font>\n",
        "---\n",
        "**Instructions:**\n",
        "- Return the answer in PDF and Jupyter Notebook formats.\n",
        "\n",
        "- Return latest on<font color='red'> Sunday Sep. 28 at 23.50</font> via Moodle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04ddc4bc-9959-4401-bcdf-67805939d106",
      "metadata": {},
      "source": [
        "## Ex 3.1. CAMERA MATRIX (4 p)\n",
        "Please find the 3 sets of images provided in the Moodle page of this week's exercise. The three image sets (A,B and C) contain pictures taken of a camera calibration checkerboard pattern with a square size of 3cm.\n",
        "\n",
        "1. Write a function using built-in OpenCV methods that computes the intrinsic camera calibration matrix and distortion matrix from a given set of calibration images.\n",
        "\n",
        "2. Apply that function to all three sets of images and observe the results. Based on the results for the intrinsic and distortion matrix, discuss what type of camera or lens was used to capture the different image sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cc269a8a-57a3-459f-a302-bb36dc894fc0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images A:\n",
            "[[3440.39368922    0.         1606.29988893]\n",
            " [   0.         3438.45417918 1924.66009328]\n",
            " [   0.            0.            1.        ]]\n",
            "[[ 0.20366683 -0.89153645 -0.00669971  0.00976955  2.01646508]]\n",
            "\n",
            "Images B:\n",
            "[[1707.80309794    0.         1477.72313398]\n",
            " [   0.         1704.50627491 1989.87414786]\n",
            " [   0.            0.            1.        ]]\n",
            "[[-0.00571016  0.01908926 -0.00171761 -0.00362864 -0.00341251]]\n",
            "\n",
            "Images C:\n",
            "[[7567.62288237    0.         1286.81980849]\n",
            " [   0.         7594.3697275  2015.22602575]\n",
            " [   0.            0.            1.        ]]\n",
            "[[ -0.17232859   9.05885973  -0.01414277  -0.00978838 -61.28880592]]\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "\n",
        "def calibrate(img_path):\n",
        "    matrix = None\n",
        "    distortion = None\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
        "    points_row, points_col = (6, 8) # number of inner corners is number of squares - 1 in any given dimension\n",
        "    square_size = 0.03 # in meters\n",
        "    objp = np.zeros((points_col*points_row, 3), np.float32)\n",
        "    objp[:,:2] = np.mgrid[0:points_row, 0:points_col].T.reshape(-1,2) * square_size\n",
        " \n",
        "    # Arrays to store object points and image points from all the images.\n",
        "    objpoints = [] # 3d point in real world space\n",
        "    imgpoints = [] # 2d points in image plane.\n",
        "\n",
        "    images = glob.glob(\"{}/*.jpeg\".format(img_path))\n",
        "\n",
        "    for filename in images:\n",
        "        image = cv2.imread(filename)\n",
        "        grayColor = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "         \n",
        "        # Find the chess board corners\n",
        "        ret, corners = cv2.findChessboardCorners(grayColor, (points_row, points_col), None)\n",
        "    \n",
        "        # If found, add object points, image points (after refining them)\n",
        "        if ret == True:\n",
        "            objpoints.append(objp)\n",
        "    \n",
        "            corners2 = cv2.cornerSubPix(grayColor, corners, (11,11), (-1,-1), criteria)\n",
        "            imgpoints.append(corners2)\n",
        "\n",
        "    ret, matrix, distortion, rvecs, tvecs = cv2.calibrateCamera(\n",
        "        objpoints, imgpoints, grayColor.shape[::-1], None, None\n",
        "    )\n",
        "\n",
        "    return matrix, distortion\n",
        "\n",
        "\n",
        "print(\"Images A:\")\n",
        "matrix, distortion = calibrate(\"images_A\")\n",
        "print(matrix)\n",
        "print(distortion)\n",
        "print()\n",
        "\n",
        "print(\"Images B:\")\n",
        "matrix, distortion = calibrate(\"images_B\")\n",
        "print(matrix)\n",
        "print(distortion)\n",
        "print()\n",
        "\n",
        "print(\"Images C:\")\n",
        "matrix, distortion = calibrate(\"images_C\")\n",
        "print(matrix)\n",
        "print(distortion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a2c660e-8752-4c6a-afe2-077b01186e71",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5dfdf103-ef14-4321-90dd-ee49c3ed3073",
      "metadata": {},
      "source": [
        "## Ex 3.2. HOUGH TRANSFORM (4 p)\n",
        "1. Ask ChatGPT to explain the main points about Hough Transform using the key word \"Hough Transform\". Analyse the answer by reflecting it to the course material related to Hough Transform: Were all main points in the course material addressed? if no, please shortly conclude the answers of ChatGPT and our course materials regarding the Hough Transform, and list the differences of answers between our course material and ChatGPT.\n",
        "2. We provided you example images from the Berkeley Segmentation Dataset. Implement the Hough Transform (the most basic version) for line detection by yourself (You may only use the openCV function to check if your solution is correct). Instead of using the default parameters, customize the parameters (e.g., resolution of the parameter space, threshold values for line detection, minimum line length, maximum line gap) to optimize edge detection for the given set of images. Report your observations: which parameters influenced which behaviour in the output? Also report which parameter configuration resembles closest an object segmentation in the test images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7107da6-a27d-4117-a0f1-20aa5fd364c8",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#2)\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "def apply_hough_transform(image):\n",
        "    ...\n",
        "\n",
        "\n",
        "images = glob.glob(\"segmentation_images/*.jpg\")\n",
        "\n",
        "for filename in images:\n",
        "    image = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61aab545-be21-4abf-81a2-559d0fccb300",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
